## Chapt 2: Perceptron
1. Perceptron = sign(W*X + b)
2. Loss function: (误分类点到分类超平面的总距离)
![loss](https://i.loli.net/2019/02/18/5c6a6982b4f1a.png)
3. 梯度下降:
![gd](https://i.loli.net/2019/02/18/5c6a6a35c6b12.png)

## chapt 3: KNN and KDT
1. KNN: 线性搜索，即对比目标点与数据集内所有点的距离
2. KDT: 将数据集以Tree的结构存储，方便快速搜索
3. KDT构造(递归):
![kdt](https://i.loli.net/2019/02/18/5c6a6bc4c6dc9.png)

4. KDT搜索(递归):
* 在kd树中找出包含目标点x的叶结点：从根结点出发，递归的向下访问kd树。若目标点当前维的坐标值小于切分点的坐标值，则移动到左子结点，否则移动到右子结点。直到子结点为叶结点为止；
* 以此叶结点为“当前最近点”；
* 递归的向上回退，在每个结点进行以下操作(下面两步操作似乎可以调换顺序)：
    + 如果该结点保存的实例点比当前最近点距目标点更近，则以该实例点为“当前最近点”；
    + 当前最近点一定存在于该结点一个子结点对应的区域。检查该子结点的父结点的另一个子结点对应的区域是否有更近的点。具体的，检查另一个子结点对应的区域是否与以目标点为球心、以目标点与“当前最近点”间的距离为半径的超球体相交。如果相交，可能在另一个子结点对应的区域内存在距离目标更近的点，移动到另一个子结点。接着，递归的进行最近邻搜索。如果不相交，向上回退。
* 当回退到根结点时，搜索结束。最后的“当前最近点”即为x的最近邻点

## chapt 4: Naive Bayes
1. 朴素贝叶斯: 学习先验概率P(Y)和条件概率P(X|Y)然后计算后验概率P(Y|X)
2. 条件独立性假设: 用于分类e特征在类确定的条件下是相互独立的
![ci](https://i.loli.net/2019/02/19/5c6b61555d161.png)
3. 后验概率计算公式(即贝叶斯定理):
![bayes](https://i.loli.net/2019/02/19/5c6b61a846e44.png)
条件独立性:
![cibayes](https://i.loli.net/2019/02/19/5c6b61e157453.png)
4. 贝叶斯分类器(可以只考虑分子，因为分母是固定的):
* 后验概率最大化=期望风险最小化
![bayesclf](https://i.loli.net/2019/02/19/5c6b621201538.png)
![bayesclf2](https://i.loli.net/2019/02/19/5c6b652c69798.png)

## chapt 5: Decision Tree
1. 特征选择: 按照特征的分类能力选择，按照这一特征将训练数据集分割成子集，使得各个子集再当前条件下有最好的分类。用信息增益表征:
![gain](https://i.loli.net/2019/02/20/5c6cc7cc2df75.png)
2. ID3生成算法: 在各结点用信息增益准则选择特征
![id3_1](https://i.loli.net/2019/02/21/5c6e64a62e290.png)
![id3_2](https://i.loli.net/2019/02/21/5c6e64e4a2f89.png)
3. C4.5生成算法：ID3算法中的信息增益准则替换为信息增益比
4. 剪枝: 生成的决策树对训练集有最好的分类效果，但会产生过拟合，所以要进行剪枝，剪枝的准则是最小化损失函数:
![loss](https://i.loli.net/2019/02/21/5c6e66da49cd6.png)
其中t是树T的叶子结点，$|T|$是叶子节点的个数，$H_t(T)$是叶结点t上的经验熵
![jys](https://i.loli.net/2019/02/21/5c6e676fbf0b9.png)
$N_{tk}$是$N_t$中第k类的个数
4. CART(分类与回归树)
* 回归树: 平方误差准则
* 分类树: 基尼指数准则(基尼指数大表示更大的不确定性)
![gini](https://i.loli.net/2019/02/21/5c6e7b62237ef.png)

## chapt 6: Logistic Regression
1. Logistic Regression: $h_{\theta}(x) = y = sigmoid(wx)$
2. 用最大似然法推导Loss function:
![loss](https://i.loli.net/2019/02/22/5c6f8f1679515.png)
3. 梯度: 
![gradient](https://i.loli.net/2019/02/22/5c6f9371d2b18.png)
